---
toc: true
layout: post
description: "Thoughts on \"Trauma or Drama: A Predictive Processing Perspective on the Continuum of Stress\""
categories: [trauma, predictive-processing]
title: The Trauma Continuum
---

# The Trauma Continuum (In progress)

I am pushing this up to the blog unfinished to see how certain formatting things work in this markdown converter. Should be okay since no one knows about this blog yet.

I recently read the paper "Trauma or Drama: A Predictive Processing Perspective on the Continuum of Stress" by Valery Krupnik. I found it while searching for perspectives on trauma that are informed by predictive processing (PP). I am interested in PP because as a machine learning engineer, it resonates strongly with my intuition. I am interested in trauma because, like everyone, I have experiences from my past that I am constantly replaying and recreating in my present, and "trauma" is one of the words that shows up a lot when you explore this internal dynamic.

As an outsider to the field, my perspective is that PTSD was a "thing" for a long time, but then in the last 5 years, everyone started calling every adverse experience a trauma. "Trauma or Drama" subtextually confirms this (modulo my oversimplification). I get the feeling that there are two camps, "don't water down the meaning of trauma - we are actually okay at treating PTSD, and we will be less successful if everyone has PTSD" vs "a lot of strategies for treating PTSD work for treating people who are hung up on adverse past experiences, so why not call those experiences trauma?" In this split, I see an instance of a pattern I see everywhere: the researcher vs. the practitioner [^1]. In the categorical vs. dimensional view of trauma, I am also reminded of how categories frequently become dimensions, and how this might fit in PP [^2].

I have huge gaps in my psychology/cog sci vocabulary, so this passage was great:

> Interoceptive signals are integrated in the insular cortex, which represents the body’s internal states on a moment-by-moment basis and coordinates its homeostatic response to compensate for the disturbances and return to its “set points” (Craig, 2002, 2009). In case of failure to return to its homeostatic state under pressure from stressors, the organism undergoes allostasis (“stability through change”) (Sterling and Eyer, 1988; McEwen and Wingfield, 2003) to a new, suboptimal, homeostatic state that can lead to pathology.

I always love me some good terminology, I think I will get a lot of mileage out of "allostasis." The passage continues:

> In such an outcome, the organism experiences allostatic overload, of which two types, types 1 and 2, have been identified (McEwen and Wingfield, 2003). Type 1 can be viewed as a situation where a disturbance overwhelms the organism’s coping resources, triggering an emergency response dramatically curtailing functions non-essential for immediate survival. Type 2 refers to a situation of a chronic stressor pressure that creates a drift away from the initial homeostatic state without triggering an emergency response.

Thresholds are a good way to turn spectrums into categories. 


## Notes

FUCK focus, Sam. The continuous vs the discrete, symbol manipulation vs linear algebra on embeddings (which is actually somehow a spectrum), theorist vs practitioner... all of these as different levels of generative models (PP).


## Footnotes

[^1]: I first saw this as a physics student, where the split is theorists vs. experimentalists. However, I didn't see this divide as interesting until reading ["Statistical Modeling, the Two Cultures"](https://projecteuclid.org/euclid.ss/1009213726). The abstract of this paper is so :fire: that I have to include it in full:

> There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.

Another great example of this practitioner/theorist split is when Peter Norvig (a big name in AI) tore apart some of Noam Chomsky's ideas (a big name in linguistics) about linguistics in ["On Chomsky and the Two Cultures of Statistical Learning"](http://norvig.com/chomsky.html). This is part of a broader dynamic which is of interest to me in my career; the way in which linguists have mostly missed the boat on the biggest revolution in our ability to process language algorithmically (via deep learning models). The reason linguists missed the deep learning revolution feels very similar to the reason that PhD statisticians mostly missed the algorithmic modeling boat: they were very attached to having "interpretable" models. By this, I mean they have a strong preference for models that can be represented as a short string of symbols, as opposed to (as is common nowadays in machine learning) millions or billions of parameters to be multiplied and added.

[^2]: As patterns move up the DAG from pure input to high-level representation, they collapse into categories. A binary category is one bit of information; it is highly compressed. As our understanding becomes more nuanced, we interact with lower-on-the-DAG representations, which are less-compressed and closer to the object being modeled. 